---
output:
  pdf_document: default
  html_document: default
  fig_width: 6 
  fig_height: 3
---

```{r setting environment, echo=FALSE, warning=FALSE, message=FALSE}
library(dplyr)
library(caret)
rm(list=ls())
```
# Executive summary
The question was raised if mpg of 1974 cars was influenced by transmission type. To answer the question a linear model was build based on mtcars data using a linear model that showed the relation between mpg on the one hand and weight, hp and transmission type on the other. It was shown that after compensating for weight and hp there was no statistical significant relation between mpg and transmission type.

# Data preparation and assumptions
Data is taken form mtcars. Mpg is the variable to be predicted. It was chosen to drop the 0.25 mile time (qsec) from the input variables becaus it is, like mpg, an outcome of the car's specs and not a design variable.

The variables cyl, vs, am, gear and carb are taken as factors. The other variables as continuous variable.
```{r data preparation, echo=FALSE}
df <- mtcars
df$cyl <- as.factor(df$cyl)
df$vs <- as.factor(df$vs)
df$am <- as.factor(df$am)
df$gear <- as.factor(df$gear)
df$carb <- as.factor(df$carb)

df_plot <- df

df_fit <- df
df_fit$y <- df_fit$mpg
df_fit <- df_fit[, which('mpg' != colnames(df_fit))]
df_fit <- df_fit[, which('qsec' != colnames(df_fit))]
```

# Exploratory analysis
When plotting a histogram of the mpg it can be seen that it looks a bit like a normal distribution with extremes at both sides of the spectrum. The lowest outliers have an mpg below 11 and the larger outliers have mpg above 30. When plotting the mpg against displacement and weight, it seems like the outliers follow a logical pattern so should not be discarded. Note, black are high mpg, green are middle mpg and red are low mpg)

```{r, echo=FALSE, fig.height=3, fig.width=7}
df_plot$mpg_category <- ifelse(df_plot$mpg < 11, "low", ifelse(df_plot$mpg > 30, 'high', 'medium'))
df_plot$mpg_category <- factor(df_plot$mpg_category)

par(mfrow=c(1, 3), 1)
hist(
    df_plot$mpg,
    breaks=15,
    xlab="mpg",
    ylab='Frequency',
    main="Histogram of miles per gallon"
    )

plot(
    y=df_plot$mpg,
    x=df_plot$disp,
    col=df_plot$mpg_category,
    xlab="Displacement",
    ylab='mpg',
    main="mpg vs displacement"
    )

plot(
    y=df_plot$mpg,
    x=df_plot$wt,
    col=df_plot$mpg_category,
    xlab="Weight",
    ylab='mpg',
    main="mpg vs weight"
    )
```

# Model selection

## Model type selection
The goal is to quantify the relation between mgp and transmission type. To do so the decision was made to use a linear model with at max polynomials to the third degree.  Because mpg is continuous, there is no need for a generalized linear model.

## Model selection strategy
Model selection is an iterative process

1. Use lasso method (discussed in course 8 of the data science track) to find the next best value for a prediction
2. Use the new variable to fit the residual. For factors just us the factor itself -1, for continous variables test polinomials to the  first second and third degree to fit and use anova (with a alpah value < 0.1) to select the best fit
3. Add the selected polinomail to the model fitting mpg and test if the new fit is better than the pervious one using anova
4. If the new fit is better then the previous one then update the residual to be fitted and remove the new fitting variable from the variables it can be fitted with

```{r, echo=FALSE, evaluate=FALSE}
lasso_next_variable <- function(df_lasso){
    lasso_fit <- train(y ~ .-1, data=df_lasso, method='lasso')
    next_variable <- lasso_fit$finalModel$actions[1]
    print(next_variable)    
}
```

```{r, echo=FALSE, evaluate=FALSE}
find_fit_type <- function(df, x){
    if (class(x) == "factor"){
        print('Use regular fit because the fitted value is a factor')
        return(lm(y ~ x - 1, data=df_fit))
    }
    fit1 <- lm(y ~ x, data=df_fit)
    fit2 <- lm(y ~ x + I(x^2), data=df)
    fit3 <- lm(y ~ x + I(x^2) + I(x^3), data=df)
    anova_outcome <- anova(fit1, fit2, fit3)
    anova_outcome_short <- c(
        second_degree=anova_outcome$`Pr(>F)`[1],
        third_degree=anova_outcome$`Pr(>F)`[2]
        )
    print(anova_outcome_short)

    
    if (is.na(anova_outcome$`Pr(>F)`[2])){
        print('anova for second degree polynomial is NA so use first degree')
        return(fit1)
    }
    else if (anova_outcome$`Pr(>F)`[3] < 0.1){
        print('A third degree polynomial is the best fit')
        return(fit3)
    }
    else if (anova_outcome$`Pr(>F)`[2] < 0.1){
        print('A second degree polynomial is the best fit')
        return(fit2)
    }
    else {
        print('A first degree polynomial is the best fit')
        return(fit1) 
    }
}
```

```{r, echo=FALSE, evaluate=FALSE}
update_df_fit <- function(df_fit, fitted_column, fit) {
    df_fit$y <- df_fit$y - predict(fit)
    columns_to_keep <- colnames(df_fit)[which(fitted_column != colnames(df_fit))]
    df_fit <- df_fit[, columns_to_keep]
}
```

In the first iteration (see full calculation in the appendix) the lasso algorithm indicated the weight as the best fitting variable. The update_df_fit indicated the polynomial of second degree to outperform the polynomial of the first degree (Pr(>F) value of 0.0033 indicating a 0.3% chance of using the second degree polynomial while it is in fact overfitting the data).

In the second iteration the lasso algorithm indicated the hp as the second input variable. The first degree polynomial was indicated as the best as the second degree gave NA in anova and the third indicated a 64% chance of overfitting. Using anova again the fit based on weight, weight ^2 and hp was a better fit than weight and weight ^2 only with a Pr(>F) of 0.0021 (a 0.2 percent chance of using the algorithm while it is in fact overfitting the data).

In the third iteration the transmission type (a factor) was added (fitting mpg with weight, weight^2, hp and aim). Anova showed that there was a 84% chance (Pr(>F) = 0.84) that this algorithm was overfitting the data. When looking at the fit summary the transmission type was shown to have a Pr(|t|) value of 0.84 indicating agian a bad estimator to the data (see appendix for full results).

# Conclusions
After compensating for weight and hp, manual or automatic transition has no  statistica significant impact on fuel consumption.

Did the student do a residual plot and some diagnostics?

# Appendix 1 Extended approach and results

In the first iteration it can be seen that the lasso algorithm defines weight as the variable to be used to fit.
```{r, message=FALSE, warning=FALSE}
next_var <- lasso_next_variable(df_fit)
```

A second degree polynomial is shown to be the best fit as it has a Pr(>F) value in anova of only 0.00022 and the third degree polynomial has a Pr(>F) value of 0.47 (way larger than a boundary value of 0.1).
```{r, warnings=FALSE}
fitted_column <- 'wt'
best_fit <- find_fit_type(df_fit, df_fit$wt)
```

Next the y value in df_fit is updated to become the residual after the fit against weight.
```{r, warnigns=FALSE}
df_fit <- update_df_fit(df_fit, fitted_column, best_fit)
```

Next hp is shown to be the next best value to fit.
```{r, message=FALSE, warning=FALSE}
next_var <- lasso_next_variable(df_fit)
```
Using anova it is shown that a first degree polynomial is the best fit (the second and third degree have Pr(>F) values higher than 0.1).
```{r, warnings=FALSE}
best_fit <- find_fit_type(df_fit, df_fit$hp)
```

Using anova it is shown that there is only a 0.2% chance that we should reject the hypothesis that the fit with hp is better than the fit without hp so we continue with the fit with hp.
```{r}
global_best_fit <- lm(mpg ~ wt + I(wt^2) + hp, data=df)
previous_best_fit <- lm(mpg ~ wt + I(wt^2), data=df)
anova(previous_best_fit, global_best_fit)
```
The df_fit dataframe is updated accordingly.
```{r, warning=FALSE}
fitted_column <- 'hp'
df_fit <- update_df_fit(df_fit, fitted_column, best_fit)
```

The next best value to fit is the transmission type.
```{r, warning=FALSE}
next_var <- lasso_next_variable(df_fit)
```
Because the transmission type is a factor the usual fit will be used.
```{r}
best_fit <- find_fit_type(df_fit, df_fit$am)
```

Using anova it can be seen that the chances of overfitting with automatic transmission added is 85%. way to high to assume that transmission type has significant influence on the mpg if corrected for weight of the car and hp of the engine.
```{r}
previous_best_fit <- lm(mpg ~ wt + I(wt^2) + hp + I(hp^2), data=df)
global_best_fit <- lm(mpg ~ wt + I(wt^2) + hp + I(hp^2) + am, data=df)
anova(previous_best_fit, global_best_fit)
```

If we would use the latest fit to interpret the coefficients (WHICH WE DO NOT) then we would get:
```{r}
summary(lm(mpg ~ wt + I(wt^2) + hp + I(hp^2) + am, data=df))
```
With the average value of weight (wt) and hp an automatic transmission car would burn 50 mpg. Going to manual transmission would save ~0.28 mpg. This number is however not statistically significant.

## Supportive functions

Three functions are used in the analysis and described below:

- lasso_next_variable to find the next variabl using the lasso method
- find_fit_type to find the degree of best fitting polynomial
- update_df_fit update the fitting dataframe by setting y to the outcome residual and removing the column used in the previous fit

```{r}
lasso_next_variable <- function(df_lasso){
    lasso_fit <- train(y ~ .-1, data=df_lasso, method='lasso')
    next_variable <- lasso_fit$finalModel$actions[1]
    print(next_variable)
}
```

```{r}
find_fit_type <- function(df, x){
    if (class(x) == "factor"){
        print('Use regular fit because the fitted value is a factor')
        return(lm(y ~ x - 1, data=df_fit))
    }
    fit1 <- lm(y ~ x, data=df_fit)
    fit2 <- lm(y ~ x + I(x^2), data=df)
    fit3 <- lm(y ~ x + I(x^2) + I(x^3), data=df)
    anova_outcome <- anova(fit1, fit2, fit3)
    anova_outcome_short <- c(
        second_degree=anova_outcome$`Pr(>F)`[1],
        third_degree=anova_outcome$`Pr(>F)`[2]
        )
    print(anova_outcome_short)

    if (is.na(anova_outcome$`Pr(>F)`[2])){
        print('anova for second degree polynomial is NA so use first degree')
        return(fit1)
    }
    else if (anova_outcome$`Pr(>F)`[3] < 0.1){
        print('A third degree polynomial is the best fit')
        return(fit3)
    }
    else if (anova_outcome$`Pr(>F)`[2] < 0.1){
        print('A second degree polynomial is the best fit')
        return(fit2)
    }
    else {
        print('A first degree polynomial is the best fit')
        return(fit1)
    }
}
```

```{r}
update_df_fit <- function(df_fit, fitted_column, fit) {
    df_fit$y <- df_fit$y - predict(fit)
    columns_to_keep <- colnames(df_fit)[which(fitted_column != colnames(df_fit))]
    df_fit <- df_fit[, columns_to_keep]
}
```

# Appendix 2 Residual analysis
In the plots below it can be seen that the residual devaites a bit from the normal distribution as it tilts slightly to the left. It does however seem like there is littel relation between mpg and the residual indicating that most significant features are well approximated.

```{r, fig.height=3, fig.width=7, echo=FALSE}
par(mfrow=c(1, 2))
used_fit <- lm(mpg ~ wt + I(wt^2) + hp + I(hp^2), data=df)
plot(
    df$mpg,
    used_fit$residuals,
    xlab='residuals',
    ylab='mpg',
    main="residual variation with mpg"
    )
hist(
    used_fit$residuals,
    xlab='residual values',
    main='Histogram of residual values'
    )
```